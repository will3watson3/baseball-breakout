


# === Imports ===
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import joblib
import os

from xgboost import XGBClassifier, plot_importance
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    classification_report,
    roc_auc_score,
    average_precision_score,
)

pd.set_option("display.max_columns", 100)
print("‚úÖ Imports complete.")



# === Load breakout dataset ===
DATA_PATH = "../outputs/combined_breakouts_and_nonbreakouts_2022_2025_ratio_plus_raw.csv"

df = pd.read_csv(DATA_PATH)
print("Rows:", len(df))
print("Columns:", len(df.columns))
print(df["season"].value_counts().sort_index())
df.head(3)




# === Split into train/test sets ===
train = df[df["season"].between(2022, 2024)].copy()
test = df[df["season"] == 2025].copy()

# Drop label & non-feature columns
label_col = "breakout_label"
non_features = ["season", label_col]
kept = [c for c in df.columns if c not in non_features]

X_train = train[kept].fillna(0).copy()
y_train = train[label_col].astype(int).copy()

X_test = test[kept].fillna(0).copy()
y_test = test[label_col].astype(int).copy()

print(f"‚úÖ Train shape: {X_train.shape} | Test shape: {X_test.shape}")
print(f"Positives in train: {y_train.sum()} / {len(y_train)}")




# === Feature cleaning helpers ===
def dedupe_cols(df):
    """Remove duplicate column names (keep first occurrence)."""
    return df.loc[:, ~df.columns.duplicated()]

def clean_features(X):
    """Remove duplicates, ensure numeric dtypes, and fill NaNs."""
    X = dedupe_cols(X)
    X = X.fillna(0)
    X = X.select_dtypes(include=[np.number])
    return X

print("‚úÖ Feature cleaning utilities loaded.")



# === Clean & validate features ===
X_train_clean = clean_features(X_train.copy())
X_test_clean = clean_features(X_test.copy())

print("Shapes after clean:", X_train_clean.shape, X_test_clean.shape)
print("Any duplicate columns?", X_train_clean.columns.duplicated().any(), X_test_clean.columns.duplicated().any())
print("All numeric:", all(np.issubdtype(dt, np.number) for dt in X_train_clean.dtypes))



# === Train Logistic Regression + XGBoost (safe version) ===
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# Ensure feature matrices exist
assert "X_train_clean" in locals() and "y_train" in locals(), "Run data prep cells first."

# Reuse existing models if they already exist
try:
    logreg
    print("‚ÑπÔ∏è Logistic Regression model already exists ‚Äî skipping refit.")
except NameError:
    print("‚öôÔ∏è Training Logistic Regression...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_clean)
    X_test_scaled = scaler.transform(X_test_clean)
    logreg = LogisticRegression(max_iter=200, solver="lbfgs")
    logreg.fit(X_train_scaled, y_train)
    print("‚úÖ Logistic Regression fitted.")

try:
    xgb
    print("‚ÑπÔ∏è XGBoost model already exists ‚Äî skipping refit.")
except NameError:
    print("‚öôÔ∏è Training XGBoost...")
    xgb = XGBClassifier(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=5,
        subsample=0.8,
        colsample_bytree=0.8,
        eval_metric="logloss",
        random_state=42,
    )
    xgb.fit(X_train_clean, y_train)
    print("‚úÖ XGBoost fitted successfully.")




# === Evaluation helper (safe version) ===
from sklearn.metrics import classification_report, roc_auc_score, average_precision_score

def evaluate_model(name, model, X, y):
    preds = model.predict(X)
    probas = model.predict_proba(X)[:, 1] if hasattr(model, "predict_proba") else preds
    print(f"\nüìà {name}")
    print(classification_report(y, preds))
    print("ROC AUC:", round(roc_auc_score(y, probas), 4))
    print("Avg Precision:", round(average_precision_score(y, probas), 4))

# Run only if models exist
if "logreg" in locals() and "xgb" in locals():
    print("‚úÖ Evaluating both models...")
    evaluate_model("Logistic Regression (scaled)", logreg, X_test_scaled, y_test)
    evaluate_model("XGBoost (raw)", xgb, X_test_clean, y_test)
else:
    print("[WARN] Models not trained yet ‚Äî run previous cell first.")



# === Interpretability: Feature Importances ===
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from xgboost import plot_importance
from sklearn.exceptions import NotFittedError

try:
    if "logreg" not in locals():
        raise NameError("logreg not defined")
    coef_df = pd.DataFrame({
        "feature": X_train_clean.columns,
        "coef": logreg.coef_[0]
    })
    coef_df["odds_ratio"] = np.exp(coef_df["coef"])
    print("\nTop Logistic Regression Odds Ratios:")
    display(coef_df.sort_values("odds_ratio", ascending=False).head(10))
except Exception as e:
    print(f"[WARN] Logistic Regression not fitted yet: {e}")

try:
    if "xgb" not in locals():
        raise NameError("xgb not defined")
    plt.figure(figsize=(8, 6))
    plot_importance(xgb, max_num_features=10)
    plt.title("XGBoost Top Features")
    plt.show()
except Exception as e:
    print(f"[WARN] Could not compute XGBoost importances: {e}")



# === Feature importance + odds ratios (safe, tidy) ===
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.exceptions import NotFittedError
from xgboost import plot_importance

# 1) Logistic Regression odds ratios (per +1 SD)
try:
    _ = logreg.coef_
    coef_df = pd.DataFrame({"feature": kept, "coef": logreg.coef_[0]})
    coef_df["odds_ratio"] = np.exp(coef_df["coef"])
    print("Top (LogReg odds ratios):")
    display(coef_df.sort_values("odds_ratio", ascending=False).head(15))
except Exception as e:
    print("[warn] Logistic Regression not fitted yet. Run the fit cell first.")
    print(e)

# 2) XGBoost feature importance (gain) with proper names
try:
    booster = xgb.get_booster()  # raises NotFittedError if not fitted

    # Raw importance dicts
    gain_dict   = booster.get_score(importance_type="gain")
    weight_dict = booster.get_score(importance_type="weight")
    cover_dict  = booster.get_score(importance_type="cover")

    # Assemble table
    keys = set().union(gain_dict.keys(), weight_dict.keys(), cover_dict.keys())
    imp = pd.DataFrame(
        [{"feature": k,
          "gain":   gain_dict.get(k, 0.0),
          "weight": weight_dict.get(k, 0),
          "cover":  cover_dict.get(k, 0.0)} for k in keys]
    )

    # Map f0,f1,‚Ä¶ to actual column names if needed
    if all(isinstance(f, str) and f.startswith("f") and f[1:].isdigit() for f in imp["feature"]):
        # Use the same feature order you trained with; here we assume X_train_clean
        fmap = {f"f{i}": name for i, name in enumerate(X_train_clean.columns)}
        imp["feature"] = imp["feature"].map(fmap)

    imp = imp.sort_values("gain", ascending=False).reset_index(drop=True)
    print("\nTop XGBoost features (by gain):")
    display(imp.head(25))

    # Plot (gain)
    ax = plot_importance(xgb, importance_type="gain",
                         max_num_features=min(25, X_train_clean.shape[1]))
    ax.set_title("XGBoost Feature Importance (gain)")
    plt.show()

except NotFittedError as e:
    print("[warn] XGBoost not fitted yet. Run the fit cell first.")
    print(e)
except Exception as e:
    print("[warn] Could not compute XGBoost importances.")
    print(e)




# === Save artifacts ===
ARTIFACT_DIR = "../artifacts"
os.makedirs(ARTIFACT_DIR, exist_ok=True)

joblib.dump(logreg, f"{ARTIFACT_DIR}/logreg_model.pkl")
joblib.dump(xgb, f"{ARTIFACT_DIR}/xgb_model.pkl")
joblib.dump(scaler, f"{ARTIFACT_DIR}/scaler.pkl")

print("‚úÖ Saved models to:", ARTIFACT_DIR)



# === Save test predictions for next notebook ===
test["breakout_pred_xgb"] = xgb.predict_proba(X_test_clean)[:, 1]
test["breakout_pred_logreg"] = logreg.predict_proba(X_test_scaled)[:, 1]

output_path = "../outputs/breakout_predictions_2025.csv"
test.to_csv(output_path, index=False)
print(f"‚úÖ Predictions exported ‚Üí {output_path}")



# === Export feature list used for training ===
import json, os

ARTIFACT_DIR = "../artifacts"
os.makedirs(ARTIFACT_DIR, exist_ok=True)

# Capture the columns your model trained on
feature_list = list(X_train.columns)

with open(f"{ARTIFACT_DIR}/feature_list.json", "w") as f:
    json.dump(feature_list, f, indent=2)

print(f"‚úÖ Saved feature_list.json with {len(feature_list)} features.")
print("Example features:", feature_list[:10])



# === Export training feature medians for use during prediction ===
training_feature_stats = {
    "medians": X_train.median(numeric_only=True).to_dict()
}

with open(f"{ARTIFACT_DIR}/training_feature_stats.json", "w") as f:
    json.dump(training_feature_stats, f, indent=2)

print(f"‚úÖ Saved training_feature_stats.json with {len(training_feature_stats['medians'])} medians.")




